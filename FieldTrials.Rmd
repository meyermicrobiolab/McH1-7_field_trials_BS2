---
title: "McH1-7 Field Trials"
author: "J. Meyer"
date: "2024-03-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load libraries

```{r, echo=FALSE}
library(dada2)
library(ggplot2)
library(phyloseq)
library(vegan)
library(knitr)
library(ALDEx2)
library(CoDaSeq)
library(dplyr)
library(randomcoloR)
writeLines(capture.output(sessionInfo()), "sessionInfo.txt")
```

## Quality-filter the sequencing reads and create Amplicon Sequence Variant (ASV) tables with DADA2

Put unjoined R1 and R2 fastq files, with adaptors and primers previously removed with cutadapt into a directory for DADA2. Here, our forward and reverse fastq filenames have format: SAMPLENAME_R1_cut.fastq.gz and SAMPLENAME_R2_cut.fastq.gz

*****If you have samples from multiple sequencing runs, you need to determine the sequence variants for each run separately, then merge the ASV tables.
Here is the dada2 page on merging runs: https://benjjneb.github.io/dada2/bigdata_paired.html

```{r, echo=FALSE}
#### 1st sequencing run for BS2
path <- "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2107_BS2"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats_NS2107_BS2.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2107_BS2/seqtab.rds") 
```


```{r, echo=FALSE}
#### 2nd sequencing run for BS2
path <- "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2172_BS2"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats_NS2172_BS2.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2172_BS2/seqtab.rds")
```


```{r, echo=FALSE}
#### 3rd sequencing run for BS2
path <- "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2211_BS2"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats_NS2211_BS2.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2211_BS2/seqtab.rds")
```


```{r, echo=FALSE}
#### 4th sequencing run for BS2
path <- "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2408_BS2"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "-V4"), `[`, 1)

# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats_NS2408_BS2.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2408_BS2/seqtab.rds")

##### Run NS2408 had six samples (all from October collection) with fewer than 1,000 tabled reads. Remove these samples and rerun dada2.
# samples removed: BS-2080-HD-O, BS2-2460-HD-O, BS2-2478-HD-O, BS2-2077-DD-O, BS2-2077-HD-O, BS2-2065-HH-O 
# don't forget to remove the names from the metadata file too
```

```{r, echo=FALSE}
#### 1st sequencing run for BS3
path <- "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2661_BS3"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "-515rcbc"), `[`, 1)

# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats_NS2661_BS3.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2661_BS3/seqtab.rds")



##### Run NS2661 had five (non-blank) samples (all from August collection) with fewer than 1,000 tabled reads. Remove these samples and rerun dada2.
# samples removed: BS3-2265-HD-A, BS3-2265-DD-A, BS3-2365-HD-A, BS3-2629-HD-A, BS3-2267-HH-A
# don't forget to remove the names from the metadata file too
```


```{r, echo=FALSE}
#### 2nd sequencing run for BS3 and MK48
path <- "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2696_BS3"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "-515rcbc"), `[`, 1)

# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats_NS2696_BS3.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2696_BS3/seqtab.rds")
```



Merge sequence tables from multiple runs

```{r, echo=FALSE}
st1 <- readRDS("~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2107_BS2/seqtab.rds")
st2 <- readRDS("~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2172_BS2/seqtab.rds") 
st3 <- readRDS("~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2211_BS2/seqtab.rds") 
st4 <- readRDS("~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2408_BS2/seqtab.rds") 
st5 <- readRDS("~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2661_BS3/seqtab.rds") 
st6 <- readRDS("~/Documents/McH1-7_field_trials_BS2andBS3/cutadapt_NS2696_BS3/seqtab.rds")
st.all <- mergeSequenceTables(st1, st2, st3, st4, st5, st6, repeats="sum") 

#Remove chimeric sequences:
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
# 421 samples and 26,295 taxa
sum(seqtab.nochim)/sum(st.all)
# proportion of non-chimeric taxa: 0.9640057 (4% of ASVs were removed as chimeras)

# SAVE the non-chimeric sequence variant table SO YOU DON'T HAVE TO REPEAT ALL OF THE ABOVE STEPS
saveRDS(seqtab.nochim, file="~/Documents/McH1-7_field_trials_BS2andBS3/probiotics.rds")

# if you need to read back in the rds file:
# seqtab.nochim <- readRDS("~/Documents/McH1-7_field_trials_BS2andBS3/probiotics.rds")
```




## Assign taxonomy in DADA2

Make sure the taxonomy reference database is in your working directory. Keep the database file gzipped. Adjust path name below. This step is very time consuming.

When taxonomy assignment is complete, we will use base R and phyloseq to clean up the taxonomy table. First, we will replace NAs and empty cells with the lowest taxonomy classification available. Second, we will use phyloseq to remove reads that are classified as Eukaryotes or unclassified at the domain level (ie, we are keeping only Bacteria and Archaea because that is what our primers target).

```{r, echo=FALSE}
taxa <- assignTaxonomy(seqtab.nochim, "~/Documents/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)
# FIX the NAs in the taxa table
taxon <- as.data.frame(taxa,stringsAsFactors=FALSE)
taxon$Phylum[is.na(taxon$Phylum)] <- taxon$Kingdom[is.na(taxon$Phylum)]
taxon$Class[is.na(taxon$Class)] <- taxon$Phylum[is.na(taxon$Class)]
taxon$Order[is.na(taxon$Order)] <- taxon$Class[is.na(taxon$Order)]
taxon$Family[is.na(taxon$Family)] <- taxon$Order[is.na(taxon$Family)]
taxon$Genus[is.na(taxon$Genus)] <- taxon$Family[is.na(taxon$Genus)]
write.table(taxon,"silva_taxa_table.txt",sep="\t",col.names=NA)
write.table(seqtab.nochim, "silva_otu_table.txt",sep="\t",col.names=NA)
```


Load data into phyloseq

```{r, echo=FALSE}
# Create phyloseq object from otu and taxonomy tables from dada2, along with the sample metadata.
otu <- read.table("silva_otu_table.txt",sep="\t",header=TRUE, row.names=1)
taxon <- read.table("silva_taxa_table.txt",sep="\t",header=TRUE,row.names=1)
samples<-read.table("metadata_BS2andBS3.txt",sep="\t",header=T,row.names=1)
OTU = otu_table(otu, taxa_are_rows=FALSE)
taxon<-as.matrix(taxon)
TAX = tax_table(taxon)
sampledata = sample_data(samples)
ps <- phyloseq(otu_table(otu, taxa_are_rows=FALSE), 
               sample_data(samples), 
               tax_table(taxon))
ps 
# 26295 taxa and 421 samples 
```

Remove chloroplasts, mitochondria, and eukaryotes

```{r, echo=FALSE}
# remove chloroplasts and mitochondria and Eukaryota
get_taxa_unique(ps, "Family") #789
get_taxa_unique(ps, "Order") #448
get_taxa_unique(ps, "Kingdom") #3
ps <- subset_taxa(ps, Family !="Mitochondria")
ps <- subset_taxa(ps, Order !="Chloroplast")
ps <- subset_taxa(ps, Kingdom !="Eukaryota")
ps <- subset_taxa(ps, Kingdom !="NA")
get_taxa_unique(ps, "Family") #786
get_taxa_unique(ps, "Order") #446
get_taxa_unique(ps, "Kingdom") #2
ps
# 25137 taxa and 421 samples
```

Export tables for future reference

```{r, echo=FALSE}
# Now export cleaned otu and taxa tables from phyloseq for future reference
otu = as(otu_table(ps), "matrix")
taxon = as(tax_table(ps), "matrix")
metadata = as(sample_data(ps), "matrix")
write.table(otu,"silva_nochloronomito_otu_table.txt",sep="\t",col.names=NA)
write.table(taxon,"silva_nochloronomito_taxa_table.txt",sep="\t",col.names=NA)
```

Export ASV table with relative abundances

```{r, echo=FALSE}
# export ASV table as relative abundance
ps_ra<-transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
otu_ra = as(otu_table(ps_ra), "matrix")
write.table(otu_ra,"silva_nochloronomito_otu_table_RA.txt",sep="\t",col.names=NA)
```


## Explore data from nochloronomito tables

Now, time to explore the data. I wanted to determine ASVs in dada2 using all samples, but I will now remove the blanks and create separate phyloseq objects for BS2 and BS3 sites - I will do plotting and analysis within sites. I will remove low abundance reads within sites.

```{r, echo=FALSE}
# read data in, if needed
otu <- read.table("silva_nochloronomito_otu_table.txt",sep="\t",header=TRUE, row.names=1)
taxon <- read.table("silva_nochloronomito_taxa_table.txt",sep="\t",header=TRUE,row.names=1)
samples<-read.table("metadata_BS2andBS3.txt",sep="\t",header=T,row.names=1)
OTU = otu_table(otu, taxa_are_rows=FALSE)
taxon<-as.matrix(taxon)
TAX = tax_table(taxon)
sampledata = sample_data(samples)
ps <- phyloseq(otu_table(otu, taxa_are_rows=FALSE), 
               sample_data(samples), 
               tax_table(taxon))
ps #25137 taxa and 421 samples

# Remove sample blanks and separate sites
psnb = subset_samples(ps, Coral.tag != "blank")
psnb #25137 taxa and 410 samples
psnb <- prune_taxa(taxa_sums(psnb) > 1, psnb) 
psnb #25013 taxa and 410 samples, 124 taxa only in blanks

psBS2 = subset_samples(psnb, Site == "BS2")
psBS2 #25137 taxa and 194 samples
psBS2 <- prune_taxa(taxa_sums(psBS2) > 1, psBS2) 
psBS2 #20105 taxa and 194 samples

psBS3 = subset_samples(psnb, Site == "BS3")
psBS3 #25013 taxa and 216 samples
psBS3 <- prune_taxa(taxa_sums(psBS3) > 1, psBS3) 
psBS3 #8050 taxa and 216 samples

# export tables for BS2
otu_psBS2 = as(otu_table(psBS2), "matrix")
taxon_psBS2 = as(tax_table(psBS2), "matrix")
metadata_psBS2 = as(sample_data(psBS2), "matrix")
write.table(otu_psBS2,"silva_nochloronomito_otu_table_BS2.txt",sep="\t",col.names=NA)
write.table(taxon_psBS2,"silva_nochloronomito_taxa_table_BS2.txt",sep="\t",col.names=NA)
write.table(metadata_psBS2,"metadata_BS2.txt",sep="\t",col.names=NA)
# export relative abundances
psBS2_ra<-transform_sample_counts(psBS2, function(OTU) OTU/sum(OTU))
otu_psBS2_ra = as(otu_table(psBS2_ra), "matrix")
write.table(otu_psBS2_ra,"silva_nochloronomito_otu_table_psBS2_RA.txt",sep="\t",col.names=NA)

# export tables for BS3
otu_psBS3 = as(otu_table(psBS3), "matrix")
taxon_psBS3 = as(tax_table(psBS3), "matrix")
metadata_psBS3 = as(sample_data(psBS3), "matrix")
write.table(otu_psBS3,"silva_nochloronomito_otu_table_BS3.txt",sep="\t",col.names=NA)
write.table(taxon_psBS3,"silva_nochloronomito_taxa_table_BS3.txt",sep="\t",col.names=NA)
write.table(metadata_psBS3,"metadata_BS3.txt",sep="\t",col.names=NA)
# export relative abundances
psBS3_ra<-transform_sample_counts(psBS3, function(OTU) OTU/sum(OTU))
otu_psBS3_ra = as(otu_table(psBS3_ra), "matrix")
write.table(otu_psBS3_ra,"silva_nochloronomito_otu_table_psBS3_RA.txt",sep="\t",col.names=NA)

#explore BS2 data
ntaxa(psBS2) #20105
get_taxa_unique(psBS2, "Order") #427
get_taxa_unique(psBS2, "Class") #185

ps5_BS2<-filter_taxa(psBS2, function(x) mean(x) >5, TRUE)
ntaxa(ps5_BS2) #755
get_taxa_unique(ps5_BS2, "Order") #102
get_taxa_unique(ps5_BS2, "Class") #46

# Export files with low abundance taxa removed
otu_ps5_BS2 = as(otu_table(ps5_BS2), "matrix")
taxon_ps5_BS2 = as(tax_table(ps5_BS2), "matrix")
metadata_ps5_BS2 = as(sample_data(ps5_BS2), "matrix")
write.table(otu_ps5_BS2,"silva_nochloronomito_otu_table_ps5_BS2.txt",sep="\t",col.names=NA)
write.table(taxon_ps5_BS2,"silva_nochloronomito_taxa_table_ps5_BS2.txt",sep="\t",col.names=NA)
write.table(metadata_ps5_BS2,"metadata_ps5_BS2.txt",sep="\t",col.names=NA)
# also write out ASV table with relative abundance
ps5_BS2_ra<-transform_sample_counts(ps5_BS2, function(OTU) OTU/sum(OTU))
otu_ps5_BS2_ra = as(otu_table(ps5_BS2_ra), "matrix")
write.table(otu_ps5_BS2_ra,"silva_nochloronomito_otu_table_ps5_BS2_RA.txt",sep="\t",col.names=NA)


#explore BS3 data
ntaxa(psBS3) #8050
get_taxa_unique(psBS3, "Order") #307
get_taxa_unique(psBS3, "Class") #120

ps5_BS3<-filter_taxa(psBS3, function(x) mean(x) >5, TRUE)
ntaxa(ps5_BS3) #246
get_taxa_unique(ps5_BS3, "Order") #43
get_taxa_unique(ps5_BS3, "Class") #17

# Export files with low abundance taxa removed
otu_ps5_BS3 = as(otu_table(ps5_BS3), "matrix")
taxon_ps5_BS3 = as(tax_table(ps5_BS3), "matrix")
metadata_ps5_BS3 = as(sample_data(ps5_BS3), "matrix")
write.table(otu_ps5_BS3,"silva_nochloronomito_otu_table_ps5_BS3.txt",sep="\t",col.names=NA)
write.table(taxon_ps5_BS3,"silva_nochloronomito_taxa_table_ps5_BS3.txt",sep="\t",col.names=NA)
write.table(metadata_ps5_BS3,"metadata_ps5_BS3.txt",sep="\t",col.names=NA)
# also write out ASV table with relative abundance
ps5_BS3_ra<-transform_sample_counts(ps5_BS3, function(OTU) OTU/sum(OTU))
otu_ps5_BS3_ra = as(otu_table(ps5_BS3_ra), "matrix")
write.table(otu_ps5_BS3_ra,"silva_nochloronomito_otu_table_ps5_BS3_RA.txt",sep="\t",col.names=NA)

```

## Perform center-log-ratio transformation on ASVs and calculate Aitchison Distance and principal components

## BS2

```{r, echo=FALSE}
# use an ASV table that has been filtered to remove low-abundance ASVs, no blanks
otu <- read.table("silva_nochloronomito_otu_table_ps5_BS2.txt",sep="\t",header=TRUE, row.names=1)
taxon <- read.table("silva_nochloronomito_taxa_table_ps5_BS2.txt",sep="\t",header=TRUE,row.names=1)
samples<-read.table("metadata_ps5_BS2.txt",sep="\t",header=T,row.names=1)
OTU = otu_table(otu, taxa_are_rows=FALSE)
taxon<-as.matrix(taxon)
TAX = tax_table(taxon)
sampledata = sample_data(samples)
ps5_BS2 <- phyloseq(otu_table(otu, taxa_are_rows=FALSE), 
               sample_data(samples), 
               tax_table(taxon))
ps5_BS2 #755 taxa and 194 samples


# First, replace 0 values with an estimate (because normalization is taking log, can't have 0)
# Also transposing here, need samples as rows
d.czm <- cmultRepl(t(otu), method="CZM", label=0)
# Perform the center-log-ratio (CLR) transformation 
d.clr <- codaSeq.clr(d.czm)
# transpose matrix of CLR transformed data for ordination and dendrogram
E.clr <- t(d.clr)
# plot compositional PCA biplot (perform a singular value decomposition)
d.pcx <- prcomp(E.clr)
# calculate percent variance explained for the axis labels
pc1 <- round(d.pcx$sdev[1]^2/sum(d.pcx$sdev^2),2)
pc2 <- round(d.pcx$sdev[2]^2/sum(d.pcx$sdev^2),2)
xlab <- paste("PC1: ", pc1, sep="")
ylab <- paste("PC2: ", pc2, sep="")
#biplot(d.pcx, cex=c(0.6,0.4), var.axes=F,scale=1, xlab=xlab, ylab=ylab)
summary(d.pcx)
str(d.pcx)
screeplot(d.pcx)

# replot PCA with ggplot2 (showing samples only)
df_out <- as.data.frame(d.pcx$x)
theme_set(theme_bw()+theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()))
cols<-c("resistant"="#000000","none"="#999999","probiotic paste"="#D55E00","probiotic bag"="#E69F00","control paste"="#0072B2","control bag"="#56B4E9")
samples$Treatment<-factor(samples$Treatment,levels=c("probiotic bag","probiotic paste","control bag","control paste","none","resistant"))
samples$Collection.month<-factor(samples$Collection.month,levels=c("Aug","Oct","Jan"))


####### Use phyloseq/vegan to perform PERMANOVA on phyloseq object
# set metadata as factors
type<-as.character(samples$Treatment)
frac<-as.character(samples$Collection.month)
# permanova between groups using Aitchison distance
dist.clr <- dist(E.clr)
perm<-adonis2(dist.clr~Treatment,as(sample_data(ps5_BS2),"data.frame"))
print(perm)
```


## Perform center-log-ratio transformation on ASVs and calculate Aitchison Distance and principal components

## BS3

```{r, echo=FALSE}
# use an ASV table that has been filtered to remove low-abundance ASVs, no blanks
otu <- read.table("silva_nochloronomito_otu_table_ps5_BS3.txt",sep="\t",header=TRUE, row.names=1)
taxon <- read.table("silva_nochloronomito_taxa_table_ps5_BS3.txt",sep="\t",header=TRUE,row.names=1)
samples<-read.table("metadata_ps5_BS3.txt",sep="\t",header=T,row.names=1)
OTU = otu_table(otu, taxa_are_rows=FALSE)
taxon<-as.matrix(taxon)
TAX = tax_table(taxon)
sampledata = sample_data(samples)
ps5_BS3 <- phyloseq(otu_table(otu, taxa_are_rows=FALSE), 
               sample_data(samples), 
               tax_table(taxon))
ps5_BS3 #246 taxa and 216 samples


# First, replace 0 values with an estimate (because normalization is taking log, can't have 0)
# Also transposing here, need samples as rows
d.czm <- cmultRepl(t(otu), method="CZM", label=0)
# Perform the center-log-ratio (CLR) transformation 
d.clr <- codaSeq.clr(d.czm)
# transpose matrix of CLR transformed data for ordination and dendrogram
E.clr <- t(d.clr)
# plot compositional PCA biplot (perform a singular value decomposition)
d.pcx <- prcomp(E.clr)
# calculate percent variance explained for the axis labels
pc1 <- round(d.pcx$sdev[1]^2/sum(d.pcx$sdev^2),2)
pc2 <- round(d.pcx$sdev[2]^2/sum(d.pcx$sdev^2),2)
xlab <- paste("PC1: ", pc1, sep="")
ylab <- paste("PC2: ", pc2, sep="")
#biplot(d.pcx, cex=c(0.6,0.4), var.axes=F,scale=1, xlab=xlab, ylab=ylab)
summary(d.pcx)
str(d.pcx)
screeplot(d.pcx)

# replot PCA with ggplot2 (showing samples only)
df_out <- as.data.frame(d.pcx$x)
theme_set(theme_bw()+theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()))
cols<-c("resistant"="#000000","none"="#999999","probiotic paste"="#D55E00","probiotic bag"="#E69F00","control paste"="#0072B2","control bag"="#56B4E9")
samples$Treatment<-factor(samples$Treatment,levels=c("probiotic bag","probiotic paste","control bag","control paste","none","resistant"))
samples$Collection.month<-factor(samples$Collection.month,levels=c("Aug","Oct","Jan"))


####### Use phyloseq/vegan to perform PERMANOVA on phyloseq object
# set metadata as factors
type<-as.character(samples$Treatment)
frac<-as.character(samples$Collection.month)
# permanova between groups using Aitchison distance
dist.clr <- dist(E.clr)
perm<-adonis2(dist.clr~Treatment,as(sample_data(ps5_BS2),"data.frame"))
print(perm)
```


Stacked bar charts
```{r, echo=FALSE}
# load in data and create phyloseq object
otu <- read.table("silva_nochloronomito_otu_table_ps5_BS2.txt",sep="\t",header=TRUE, row.names=1)
taxon <- read.table("silva_nochloronomito_taxa_table_ps5_BS2.txt",sep="\t",header=TRUE,row.names=1)
samples<-read.table("metadata_ps5_BS2.txt",sep="\t",header=T,row.names=1)
OTU = otu_table(otu, taxa_are_rows=FALSE)
taxon<-as.matrix(taxon)
TAX = tax_table(taxon)
sampledata = sample_data(samples)
ps <- phyloseq(otu_table(otu, taxa_are_rows=FALSE), 
               sample_data(samples), 
               tax_table(taxon))
ps #755 taxa and 194 samples

samples$Treatment<-factor(samples$Treatment,levels=c("probiotic bag","probiotic paste","control bag","control paste","none","resistant"))
samples$Collection.month<-factor(samples$Collection.month,levels=c("Aug","Oct","Jan"))

ps_ra<-transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps_ra #755 taxa and 194 samples
get_taxa_unique(ps_ra, "Class") #46
get_taxa_unique(ps_ra, "Order") #102
get_taxa_unique(ps_ra, "Family") #183
get_taxa_unique(ps_ra, "Genus") #322

n <- 46
# after plotting, you can re-run the next line to create a different selection of colors
palette <- distinctColorPalette(n)

pdf("barchart_Class.pdf",width=11)
p1=plot_bar(ps_ra ,fill="Class")+
  facet_grid(Condition~Collection.month,scales="free",space="free")+
  geom_bar(aes(fill=Class), stat="identity",position="stack")+
  theme_bw()+
  theme(strip.text=element_text(face="bold", size=12))+
  theme(axis.text.x=element_text(size=12))+
  theme(axis.text.y=element_text(size=12))+
  scale_fill_manual(values=palette)+
  #theme(axis.title.x = element_blank())+
  theme(legend.position = "bottom")
p1
dev.off()


```



